---
title: Ebuss Product Recommender
emoji: ðŸ›’
colorFrom: indigo
colorTo: purple
sdk: docker
app_port: 7860
---

# Sentiment-Based Product Recommendation System

## Ebuss E-Commerce Platform

A hybrid recommendation system that combines collaborative filtering with sentiment analysis. The idea is simple: don't just recommend what similar users bought - recommend what they actually *liked*.

**Live Demo:** https://sentimentbasedproductrecommendationsystem.onrender.com

---

## What This Project Does

Most recommendation systems just look at purchase patterns. But a product might be selling well while having terrible reviews. This system fixes that by:

1. Finding products similar users liked (User-Based Collaborative Filtering)
2. Analyzing all reviews for those products (Sentiment Analysis with Random Forest)
3. Only recommending products with genuinely positive sentiment

Enter a username â†’ get 5 products that similar users loved AND have good reviews.

---

## The Tech Stack

- **Python 3.11** - because 3.13 broke some dependencies
- **Random Forest** - for sentiment classification (tried XGBoost but it was too big)
- **TF-IDF** - converting review text to features (5000 features, unigrams + bigrams)
- **User-Based CF** - better than item-based for our sparse matrix
- **Flask** - simple and gets the job done
- **Docker** - for deployment on Hugging Face Spaces

---

## Challenges I Ran Into

### Class Imbalance (9:1 ratio)

90% of reviews are positive. My first model just predicted "Positive" for everything and got 90% accuracy. Useless.

**Fix:** SMOTE to balance the training data. F1 went from 0.88 to 0.93.

### XGBoost Was Too Big

XGBoost had slightly better metrics but the model was ~200MB. Render's free tier has 512MB RAM total. App kept crashing.

**Fix:** Switched to Random Forest. Similar F1-score (~0.92), smaller footprint.

### NLTK Downloads Failing Silently

Worked locally, failed on Render. No error messages, just broken predictions.

**Fix:** Added try-except around downloads, set `quiet=True`, discovered I needed `punkt_tab` for newer NLTK versions.

### Git LFS Files Not Actually Downloading

Spent hours on this. Models existed but were just LFS pointer files (tiny text files instead of actual model data).

**Fix:** Added `build.sh` with explicit `git lfs pull`. Also added file size checks at startup to catch this early.

### Item-Based CF Performed Poorly

With users rating only 1-2 products on average, the item-item similarity matrix was mostly zeros.

**Fix:** Kept both implementations but defaulted to User-Based CF.

### TF-IDF Vocabulary Mismatch

Got garbage predictions until I realized I was creating a *new* TF-IDF vectorizer for inference instead of using the trained one. Different vocabulary = wrong features.

**Fix:** Pickle the vectorizer alongside the model. Use the same instance everywhere.

---

## Project Structure

```
â”œâ”€â”€ Sentiment_Based_Product_Recommendation_System.ipynb  # All the analysis
â”œâ”€â”€ model.py                    # Prediction and recommendation logic
â”œâ”€â”€ app.py                      # Flask app
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Frontend (Ebuss branded)
â”œâ”€â”€ models/                     # Trained models (run notebook first)
â”‚   â”œâ”€â”€ sentiment_model.pkl
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl
â”‚   â”œâ”€â”€ user_similarity.pkl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample30.csv            # ~30k reviews
â”œâ”€â”€ Dockerfile                  # For HF Spaces
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## How to Run Locally

```bash
# Clone it
git clone https://github.com/muralik31/Capstone--SentimentBasedProductRecommendationSystem.git
cd "Sentiment Based Product Recommendation System"

# Setup
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Run the notebook first (generates model files)
jupyter notebook Sentiment_Based_Product_Recommendation_System.ipynb
# Execute all cells

# Then run the app
python app.py
# Visit http://localhost:7860
```

---

## Model Performance

| Model | F1-Score | Notes |
|-------|----------|-------|
| Logistic Regression | 0.93 | Best F1, but wanted ensemble |
| **Random Forest** | 0.92 | Good balance of performance + size |
| XGBoost | 0.92 | Too big for free tier deployment |
| Naive Bayes | 0.90 | Fast but lower recall |

Went with Random Forest - solid performance and actually fits in memory.

### Why User-Based CF?

| Approach | Works? | Why |
|----------|--------|-----|
| **User-Based CF** | Yes | Finds similar users, gets their high-rated products |
| Item-Based CF | Meh | Matrix too sparse, most item pairs have no common raters |

---

## API Endpoints

| Endpoint | Method | What it does |
|----------|--------|--------------|
| `/` | GET | The UI |
| `/recommend` | POST | Get recommendations (send `{"username": "..."}`) |
| `/api/users` | GET | List of valid usernames for testing |
| `/api/health` | GET | Health check + model file info |

---

## Dataset

~30,000 reviews across 271 products from ~25,000 users. Typical e-commerce stuff - lots of Clorox products, mostly positive reviews, most users only review 1-2 items.

---

## Author

Murali Kondapally  

---
